# -*- coding: utf-8 -*-
"""NLP HW 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ez8YB2q5SQZkUQdixg-xQhyODuo8GztC
"""

# =========================
# 3.1  Maximum Likelihood Estimation (MLE) for N-grams (N=1,2,3,4)
# Data paths: test/train/valid (valid unused in 3.1)
# =========================

from collections import Counter, defaultdict
from math import log, exp
from typing import List, Tuple
import io

# --- file paths ---
TRAIN_PATH = "/content/ptb.train.txt"
VALID_PATH = "/content/ptb.valid.txt"
TEST_PATH  = "/content/ptb.test.txt"

# --------- Helpers ---------
def read_corpus(path: str) -> List[List[str]]:
    """
    Read the corpus assuming each line is a tokenized sentence.
    Return: list of sentences (each a list of tokens).
    """
    sents = []
    with io.open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # simple whitespace tokenization; PTB files are pre-tokenized
            toks = line.split()
            sents.append(toks)
    return sents

def build_ngram_counts(sents: List[List[str]], n: int) -> Tuple[Counter, Counter, int]:
    """
    Build n-gram and (n-1)-gram prefix counts for MLE.
    Returns:
      ngram_counts: Counter of n-grams (as tuples)
      prefix_counts: Counter of (n-1)-grams (as tuples); empty for unigrams
      total_unigrams: total # of tokens for unigram denominator (excludes artificial <s>)
    Notes:
      - For n>=2: pad each sentence with (n-1) <s> and one </s>.
      - For unigrams: we do NOT inject <s>; we DO count </s> once per sentence,
        so test perplexity accounts for sentence termination.
    """
    ngram_counts = Counter()
    prefix_counts = Counter()
    total_unigrams = 0

    if n == 1:
        for toks in sents:
            # count sentence tokens + </s> (so generation can end)
            seq = toks + ["</s>"]
            ngram_counts.update(seq)      # unigrams
            total_unigrams += len(seq)
        return ngram_counts, prefix_counts, total_unigrams

    # n >= 2: add (n-1) start symbols + one end symbol
    pad = ["<s>"] * (n - 1)
    for toks in sents:
        seq = pad + toks + ["</s>"]
        # slide over sequence to make n-grams
        for i in range(len(seq) - n + 1):
            ng = tuple(seq[i : i + n])
            pr = tuple(seq[i : i + n - 1])
            ngram_counts[ng] += 1
            prefix_counts[pr] += 1

    return ngram_counts, prefix_counts, total_unigrams

def sentence_logprob_mle(sent: List[str], n: int,
                         ngram_counts: Counter,
                         prefix_counts: Counter,
                         total_unigrams: int) -> Tuple[float, bool, int]:
    """
    Compute log-prob of a single sentence under MLE n-gram model.
    Returns:
      logp: sum of log probs
      has_zero: True if any required prob is zero
      tokens_predicted: number of predicted tokens contributed by this sentence
    """
    if n == 1:
        seq = sent + ["</s>"]
        logp = 0.0
        has_zero = False
        for w in seq:
            c_w = ngram_counts.get(w, 0)
            if c_w == 0 or total_unigrams == 0:
                has_zero = True
                break
            p = c_w / total_unigrams
            logp += log(p)
        return logp, has_zero, len(seq)

    pad = ["<s>"] * (n - 1)
    seq = pad + sent + ["</s>"]
    logp = 0.0
    has_zero = False
    # for each next-token conditional
    tokens_predicted = 0
    for i in range(n - 1, len(seq)):
        ng = tuple(seq[i - (n - 1) : i + 1])       # length-n window
        pr = ng[:-1]                               # prefix (n-1)-gram
        c_ng = ngram_counts.get(ng, 0)
        c_pr = prefix_counts.get(pr, 0)
        if c_ng == 0 or c_pr == 0:
            has_zero = True
            break
        p = c_ng / c_pr
        logp += log(p)
        tokens_predicted += 1
    return logp, has_zero, tokens_predicted

def corpus_perplexity_mle(sents: List[List[str]], n: int,
                          ngram_counts: Counter,
                          prefix_counts: Counter,
                          total_unigrams: int):
    """
    Compute perplexity on a corpus for an MLE n-gram model.
    If any zero-probability event occurs, return float('inf').
    """
    total_logp = 0.0
    total_tokens = 0
    for sent in sents:
        lp, has_zero, k = sentence_logprob_mle(sent, n, ngram_counts, prefix_counts, total_unigrams)
        if has_zero:
            return float('inf')
        total_logp += lp
        total_tokens += k

    # perplexity = exp( - (1/M) * sum log p )
    return exp(- total_logp / max(total_tokens, 1))

# --------- Load data ---------
train_sents = read_corpus(TRAIN_PATH)
test_sents  = read_corpus(TEST_PATH)

# --------- Train and Evaluate MLE for N = 1,2,3,4 ---------
results = {}
for n in [1, 2, 3, 4]:
    # build counts (MLE parameters)
    ngram_c, prefix_c, total_uni = build_ngram_counts(train_sents, n)
    # evaluate PP on test (with INF if any zero prob)
    pp = corpus_perplexity_mle(test_sents, n, ngram_c, prefix_c, total_uni)
    results[n] = pp

# --------- Report ---------
# Note: If PP == inf, it highlights zero-probability events.
for n in [1, 2, 3, 4]:
    print(f"N={n}  MLE Test Perplexity: {results[n]}")

# =========================
# 3.2.1  Add-1 (Laplace) Smoothing for Trigram Model
# =========================

from collections import Counter
from math import log, exp
import io

# --- File paths (same as before) ---
TRAIN_PATH = "/content/ptb.train.txt"
VALID_PATH = "/content/ptb.valid.txt"
TEST_PATH  = "/content/ptb.test.txt"

def read_corpus(path):
    sents = []
    with io.open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                sents.append(line.split())
    return sents

def build_ngram_counts(sents, n):
    ngram_counts = Counter()
    prefix_counts = Counter()
    vocab = set()
    for toks in sents:
        vocab.update(toks)
        seq = ["<s>"]*(n-1) + toks + ["</s>"]
        for i in range(len(seq)-n+1):
            ng = tuple(seq[i:i+n])
            pr = ng[:-1]
            ngram_counts[ng]+=1
            prefix_counts[pr]+=1
    return ngram_counts, prefix_counts, vocab

def sentence_logprob_add1(sent, n, ngram_c, prefix_c, V):
    seq = ["<s>"]*(n-1) + sent + ["</s>"]
    logp = 0.0
    for i in range(n-1, len(seq)):
        ng = tuple(seq[i-n+1:i+1])
        pr = ng[:-1]
        c_ng = ngram_c.get(ng,0)
        c_pr = prefix_c.get(pr,0)
        p = (c_ng + 1) / (c_pr + V)
        logp += log(p)
    return logp, len(seq)-(n-1)

def corpus_perplexity_add1(sents, n, ngram_c, prefix_c, V):
    total_logp = 0.0
    total_tok = 0
    for sent in sents:
        lp, k = sentence_logprob_add1(sent, n, ngram_c, prefix_c, V)
        total_logp += lp
        total_tok += k
    return exp(-total_logp/total_tok)

# --- Train on PTB Train; Evaluate on Test ---
train_sents = read_corpus(TRAIN_PATH)
test_sents  = read_corpus(TEST_PATH)
trigram_c, bigram_c, vocab = build_ngram_counts(train_sents, 3)
V = len(vocab) + 1   # add </s>

pp_add1 = corpus_perplexity_add1(test_sents, 3, trigram_c, bigram_c, V)
print(f"Trigram Add-1 Smoothing Test Perplexity: {pp_add1:.2f}")

# =========================
# 3.2.2  Backoff and Interpolation (Trigram)
# =========================

from math import log, exp

TRAIN_PATH = "/content/ptb.train.txt"
VALID_PATH = "/content/ptb.valid.txt"
TEST_PATH  = "/content/ptb.test.txt"

# Reuse read_corpus() and build_ngram_counts() from previous cell

# --- Load data ---
train_sents = read_corpus(TRAIN_PATH)
valid_sents = read_corpus(VALID_PATH)
test_sents  = read_corpus(TEST_PATH)

uni_c, _, vocab = build_ngram_counts(train_sents, 1)
bi_c, uni_prefix, _ = build_ngram_counts(train_sents, 2)
tri_c, bi_prefix, _ = build_ngram_counts(train_sents, 3)
V = len(vocab) + 1
total_uni = sum(uni_c.values())

# ---------- Linear Interpolation ----------
def prob_interp(word, history, lambdas):
    """Return interpolated probability using trigram/bigram/unigram"""
    trigram = tuple(history[-2:] + [word])
    bigram = tuple(history[-1:] + [word])
    unigram = (word,)
    p3 = tri_c.get(trigram, 0) / bi_prefix.get(trigram[:-1], 1)
    p2 = bi_c.get(bigram, 0) / uni_prefix.get(bigram[:-1], 1)
    p1 = uni_c.get(unigram, 0) / total_uni
    return lambdas[0]*p1 + lambdas[1]*p2 + lambdas[2]*p3

def perplexity_interp(sents, lambdas):
    total_logp, total_tok = 0.0, 0
    for sent in sents:
        seq = ["<s>", "<s>"] + sent + ["</s>"]
        for i in range(2, len(seq)):
            hist = seq[i-2:i]
            w = seq[i]
            p = prob_interp(w, hist, lambdas)
            if p == 0: p = 1e-12
            total_logp += log(p)
            total_tok += 1
    return exp(-total_logp/total_tok)

# Try at least 3 lambda combos; tune on Dev
lambda_sets = [
    (0.2, 0.3, 0.5),
    (0.1, 0.3, 0.6),
    (0.3, 0.3, 0.4)
]
best_pp, best_lmb = float('inf'), None
for lset in lambda_sets:
    pp = perplexity_interp(valid_sents, lset)
    print(f"Lambdas={lset} Dev PP={pp:.2f}")
    if pp < best_pp:
        best_pp, best_lmb = pp, lset

print(f"\nâœ… Best Lambdas={best_lmb}, Dev PP={best_pp:.2f}")
final_pp = perplexity_interp(test_sents, best_lmb)
print(f"Trigram Linear Interpolation Test PP={final_pp:.2f}")

# ---------- Stupid Backoff ----------
alpha = 0.4  # typical default; can tune using Dev
def prob_backoff(word, history):
    trigram = tuple(history[-2:] + [word])
    bigram = tuple(history[-1:] + [word])
    unigram = (word,)
    if tri_c.get(trigram, 0) > 0:
        return tri_c[trigram]/bi_prefix[trigram[:-1]]
    elif bi_c.get(bigram, 0) > 0:
        return alpha * bi_c[bigram]/uni_prefix[bigram[:-1]]
    else:
        return alpha**2 * uni_c.get(unigram,0)/total_uni

def perplexity_backoff(sents):
    total_logp, total_tok = 0.0, 0
    for sent in sents:
        seq = ["<s>", "<s>"] + sent + ["</s>"]
        for i in range(2, len(seq)):
            hist = seq[i-2:i]
            w = seq[i]
            p = prob_backoff(w, hist)
            if p == 0: p = 1e-12
            total_logp += log(p)
            total_tok += 1
    return exp(-total_logp/total_tok)

pp_backoff = perplexity_backoff(test_sents)
print(f"Trigram Stupid Backoff Test PP={pp_backoff:.2f}")

# =========================
# 4.4  Qualitative Analysis: Text Generation
# =========================

import random

def generate_sentence(max_len=20, lambdas=(0.2, 0.3, 0.5)):
    sent = ["<s>", "<s>"]
    while len(sent) < max_len:
        hist = sent[-2:]
        candidates = list(vocab)
        probs = [prob_interp(w, hist, lambdas) for w in candidates]
        # Normalize
        total = sum(probs)
        probs = [p/total for p in probs]
        next_w = random.choices(candidates, weights=probs, k=1)[0]
        if next_w == "</s>":
            break
        sent.append(next_w)
    return " ".join(sent[2:])

# Generate 5 distinct sentences from best performing model
for i in range(5):
    print(f"Sentence {i+1}:", generate_sentence())